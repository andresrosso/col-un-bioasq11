{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecbff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import lxml.etree as ET\n",
    "import elasticsearch\n",
    "import elasticsearch.helpers\n",
    "import pandas as pd\n",
    "\n",
    "import functools\n",
    "import logging\n",
    "from os import path\n",
    "import yaml\n",
    "import logging\n",
    "import logging.config\n",
    "import yaml\n",
    "import codecs\n",
    "import traceback\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ce29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = path.join(path.dirname(path.abspath('logging-config.yaml')), 'logging-config.yaml')\n",
    "\n",
    "with open(log_file_path, 'r') as f:\n",
    "    log_cfg = yaml.safe_load(f.read())\n",
    "\n",
    "logging.config.dictConfig(log_cfg)\n",
    "\n",
    "logger = logging.getLogger('test')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "logger.info('BioASQ Indexing {}'.format(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a90163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict(D):\n",
    "    if isinstance(D, dict):\n",
    "        o = {}\n",
    "        for k,v in D.items():\n",
    "            x = filter_dict(v)\n",
    "            if x is not None:\n",
    "                o[k] = x\n",
    "        if len(o) == 0:\n",
    "            return\n",
    "        return o\n",
    "    else:\n",
    "        return D\n",
    "\n",
    "def text(node, pattern):\n",
    "    obj = node.find(pattern)\n",
    "    if obj is None:\n",
    "        return\n",
    "    return obj.text\n",
    "\n",
    "def as_list(node, pattern, attr=None):\n",
    "    obj = node.findall(pattern)\n",
    "    if len(obj) == 0:\n",
    "        return []\n",
    "    if attr is None:\n",
    "        return [x.text for x in obj]\n",
    "    else:\n",
    "        return [x.attrib[attr] for x in obj]\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def get_abstract(node, pattern):\n",
    "    obj = node.findall(pattern)\n",
    "    if len(obj) == 0:\n",
    "        return \"\"\n",
    "    list_abs = [x.text for x in obj]\n",
    "    if len(list_abs) > 0 and (None not in list_abs):\n",
    "        abstract_str = ''.join(list_abs)\n",
    "        return unicodedata.normalize('NFKD', abstract_str)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_authors(c):\n",
    "    if (article := c.find(\"Article\")) is None:\n",
    "        return []\n",
    "    if (author_list := article.find(\"AuthorList\")) is None:\n",
    "        return []\n",
    "    o = []\n",
    "    def t(x,k):\n",
    "        if (n := x.find(k)) is None:\n",
    "            return \"\"\n",
    "        return n.text\n",
    "\n",
    "    for author in author_list.findall(\"Author\"):\n",
    "        last = t(author,\"LastName\")\n",
    "        first = t(author,\"ForeName\")\n",
    "        text = f\"{last} {first}\"\n",
    "        if text.strip():\n",
    "            o.append(text.strip())\n",
    "    return o\n",
    "\n",
    "def parse_file(path):\n",
    "    files_parsed = pd.DataFrame( columns=['file','status'] )\n",
    "    failed = pd.DataFrame(columns = ['Article', 'File', 'Result', 'Error'])\n",
    "    \n",
    "    #ChemicalList\n",
    "    with gzip.open(path, \"rb\") as h:\n",
    "        logger.info(f\"Parsing {path}....\")\n",
    "        n = 0\n",
    "        n_error = 0\n",
    "        for _,node in ET.iterparse(h, tag=\"PubmedArticle\", encoding='utf-8'):\n",
    "            try:\n",
    "                c = node.find(\"MedlineCitation\")\n",
    "                year, month, day = [text(c, f\"DateRevised/{x}\") for x in \n",
    "                        [\"Year\", \"Month\", \"Day\"]]\n",
    "                if all([year, month, day]):\n",
    "                    date = datetime.date(int(year), int(month), int(day))\n",
    "                else:\n",
    "                    date = None\n",
    "\n",
    "                article = {\n",
    "                    \"pmid\": int(text(c, \"PMID\")),\n",
    "                    \"file\": path.split('/')[-1],\n",
    "                    \"doi\": text(c, \"\"\"Article/ELocationID[@EIdType=\"doi\"]\"\"\"),\n",
    "                    \"pubdate\": date,\n",
    "                    \"authors\": get_authors(c),\n",
    "                    \"title\": text(c, \"Article/ArticleTitle\"),\n",
    "                    #\"Abstract\": text(c, \"Article/Abstract/AbstractText\"),\n",
    "                    \"abstract\": get_abstract(c, \n",
    "                        \"Article/Abstract/AbstractText\"),\n",
    "                    \"journal\": {\n",
    "                        k:text(c, f\"Article/Journal/{k}\") for k\n",
    "                        in [\"ISSN\", \"Title\", \"ISOAbbreviation\"]\n",
    "                    },\n",
    "                    \"mesh_terms\": as_list(c, \n",
    "                        \"MeshHeadingList/MeshHeading/DescriptorName\", \"UI\"),\n",
    "                    \"keywords\": as_list(c, \n",
    "                        \"KeywordList/Keyword\"),\n",
    "                }\n",
    "                n += 1\n",
    "                #article[\"Citations\"] = list(map(int, as_list(node,   \"\"\"PubmedData/ReferenceList/Reference/ArticleIdList/ArticleId[@IdType=\"pubmed\"]\"\"\")))\n",
    "                yield {\n",
    "                    \"_index\": \"pubmed2023\",\n",
    "                    \"_op_type\": \"update\",\n",
    "                    \"_id\": article[\"pmid\"],\n",
    "                    \"doc_as_upsert\": True,\n",
    "                    \"doc\": article\n",
    "                }\n",
    "                if n<0:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logger.error(\"Error processing file: {}\".format(path))\n",
    "                logging.error(traceback.format_exc())\n",
    "                n_error += 1\n",
    "                failed = pd.concat(objs = [failed, \n",
    "                                           pd.DataFrame({'Article' : article['pmid'], 'File' : path, 'Result': 0, 'Error':e.message},\n",
    "                                           index=[n])],\n",
    "                                   axis = 0)\n",
    "        files_parsed = pd.concat(objs=[files_parsed,\n",
    "                               pd.DataFrame({'file':[path], 'status':[1]})])\n",
    "        logger.info(f\"Finished {path} ({n - n_error} / {n} fully successful)\")\n",
    "        failed.to_csv('failed.csv', mode='a', index=False, header=False)\n",
    "        files_parsed.to_csv('files_processed.csv', mode='a', index=False, header=False)\n",
    "\n",
    "\n",
    "def eager_parse_file(path):\n",
    "    return list(parse_file(path))\n",
    "\n",
    "def parse_all(rootdir, ignore_files=[], ncpu=None):\n",
    "    \"\"\"\n",
    "    Parse Pubmed into records.\n",
    "    \"\"\"\n",
    "    year = str(datetime.datetime.now().year)[2:]\n",
    "    #list files to parse and ignore already parsed files\n",
    "    files_to_parse = list(sorted(glob.glob(f\"{rootdir}/pubmed{year}n*.xml.gz\")))\n",
    "    if len(ignore_files) > 0:\n",
    "        logger.info(f'Already parsed {len(ignore_files)} of {len(files_to_parse)}')\n",
    "        files_to_parse = set(files_to_parse) - set(ignore_files)\n",
    "    logger.info(f'Total files to parse {len(files_to_parse)}')\n",
    "    assert len(files_to_parse) > 0\n",
    "\n",
    "    ncpu = ncpu or max(min(4, int(mp.cpu_count())/2)+1, 1)\n",
    "    pool = mp.Pool(ncpu)\n",
    "    \n",
    "    return itertools.chain.from_iterable(pool.imap(eager_parse_file, files_to_parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30088f17",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     files_parsed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles_processed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[1;32m     14\u001b[0m     files_parsed\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{files_parsed\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m                                  files_parsed\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m]:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[43mparse_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXML_DIRECTORY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_files\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfiles_parsed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiles_parsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#logger.info(f'Total parsed files {len(it)}')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m elasticsearch\u001b[38;5;241m.\u001b[39mhelpers\u001b[38;5;241m.\u001b[39mbulk(es, it, stats_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 139\u001b[0m, in \u001b[0;36mparse_all\u001b[0;34m(rootdir, ignore_files, ncpu)\u001b[0m\n\u001b[1;32m    137\u001b[0m     files_to_parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(files_to_parse) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(ignore_files)\n\u001b[1;32m    138\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal files to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files_to_parse)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files_to_parse) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    141\u001b[0m ncpu \u001b[38;5;241m=\u001b[39m ncpu \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mint\u001b[39m(mp\u001b[38;5;241m.\u001b[39mcpu_count())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m pool \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mPool(ncpu)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index_name = \"pubmed2023\"\n",
    "es = elasticsearch.Elasticsearch(hosts=['localhost:9200'],\n",
    "            timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "#es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "#es.indices.create(index=index_name)\n",
    "\n",
    "#XML_DIRECTORY = \"/opt/bioasq/resources/pubmed_baseline_2023/test\"\n",
    "XML_DIRECTORY = \"/opt/bioasq/resources/pubmed_baseline_2023\"\n",
    "\n",
    "#Ignore already parsed files\n",
    "files_parsed = pd.DataFrame( columns=['file','status'] )\n",
    "if os.path.isfile('files_processed.csv') == True:\n",
    "    files_parsed = pd.read_csv( 'files_processed.csv' )\n",
    "    files_parsed.rename(columns={files_parsed.columns[0]:\"file\",\n",
    "                                 files_parsed.columns[1]:\"status\"}, inplace = True)\n",
    "    \n",
    "it = parse_all(XML_DIRECTORY, ignore_files = files_parsed[files_parsed.status == 1].file.values.tolist())\n",
    "#logger.info(f'Total parsed files {len(it)}')\n",
    "elasticsearch.helpers.bulk(es, it, stats_only=True)\n",
    "logger.info(\"Refreshing ES index...\")\n",
    "es.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03cc8d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (6) Could not resolve host: POST\n",
      "{\"count\":6993097,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0}}"
     ]
    }
   ],
   "source": [
    "!curl POST localhost:9200/pubmed2023/_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9790b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"pubmed2023\"\n",
    "\n",
    "def search_by_id(pmid, index=index_name):\n",
    "    pmid_query = {\n",
    "                    \"match_phrase\": {\n",
    "                        \"pmid\": int(pmid)\n",
    "                    }\n",
    "                }\n",
    "    return search_doc(index=index_name, es_query=pmid_query)\n",
    "\n",
    "def search_doc(index=index_name, es_query=None, all_docs=False):\n",
    "    es = elasticsearch.Elasticsearch(hosts=['localhost:9200'])\n",
    "    quesy = {}\n",
    "    if all_docs:\n",
    "        es_query = {\"match_all\":{}}\n",
    "    if es_query == None:\n",
    "        es_query = {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match_phrase\": { \"title\": \"\" }}\n",
    "                    ],\n",
    "                    \"filter\": [\n",
    "                        {\"range\": {\"Date\": {\"gte\": 1970}}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "    print('----',es_query,'-----')\n",
    "    result = es.search(\n",
    "                    index=index_name,\n",
    "                    body={\n",
    "                        \"_source\": [ \"ID\", \"FILE\", \"Title\", \"Author\", \"Date\", \"Abstract\", \"_score\" ],\n",
    "                        \"size\": 3,\n",
    "                        \"query\": es_query\n",
    "                    })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546313e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- {'match_phrase': {'pmid': 36283295}} -----\n",
      "{\n",
      "    \"_shards\": {\n",
      "        \"failed\": 0,\n",
      "        \"skipped\": 0,\n",
      "        \"successful\": 1,\n",
      "        \"total\": 1\n",
      "    },\n",
      "    \"hits\": {\n",
      "        \"hits\": [],\n",
      "        \"max_score\": null,\n",
      "        \"total\": {\n",
      "            \"relation\": \"eq\",\n",
      "            \"value\": 0\n",
      "        }\n",
      "    },\n",
      "    \"timed_out\": false,\n",
      "    \"took\": 3\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22100/2428000518.py:28: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  result = es.search(\n"
     ]
    }
   ],
   "source": [
    "results = search_by_id(pmid=\"36283295\") \n",
    "print(json.dumps(results, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c4298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- {'match_all': {}} -----\n",
      "{\n",
      "    \"_shards\": {\n",
      "        \"failed\": 0,\n",
      "        \"skipped\": 0,\n",
      "        \"successful\": 1,\n",
      "        \"total\": 1\n",
      "    },\n",
      "    \"hits\": {\n",
      "        \"hits\": [\n",
      "            {\n",
      "                \"_id\": \"1\",\n",
      "                \"_index\": \"pubmed2023\",\n",
      "                \"_score\": 1.0,\n",
      "                \"_source\": {},\n",
      "                \"_type\": \"_doc\"\n",
      "            },\n",
      "            {\n",
      "                \"_id\": \"2\",\n",
      "                \"_index\": \"pubmed2023\",\n",
      "                \"_score\": 1.0,\n",
      "                \"_source\": {},\n",
      "                \"_type\": \"_doc\"\n",
      "            },\n",
      "            {\n",
      "                \"_id\": \"4\",\n",
      "                \"_index\": \"pubmed2023\",\n",
      "                \"_score\": 1.0,\n",
      "                \"_source\": {},\n",
      "                \"_type\": \"_doc\"\n",
      "            }\n",
      "        ],\n",
      "        \"max_score\": 1.0,\n",
      "        \"total\": {\n",
      "            \"relation\": \"gte\",\n",
      "            \"value\": 10000\n",
      "        }\n",
      "    },\n",
      "    \"timed_out\": false,\n",
      "    \"took\": 7\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183955/3368965129.py:28: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  result = es.search(\n"
     ]
    }
   ],
   "source": [
    "results = search_doc(all_docs=True)\n",
    "print(json.dumps(results, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290839a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98329458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5aee25475e863fa768a000579985a1b8a6956af09a193c7e017c2e2b00afbf39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
