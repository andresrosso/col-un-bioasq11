{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1842742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home path : /opt/bioasq/col-un-bioasq11\n",
      "Eval path : /opt/bioasq/Evaluation-Measures\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import socket\n",
    "from elasticsearch import Elasticsearch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import nltk.data\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "import globals\n",
    "from qa_data import QAPair\n",
    "import bioasq_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d4dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_document(question, doc_id, doc_title, doc_abstract):\n",
    "    passages_ranked = []\n",
    "    chunks_title = ranking.split_chunks(doc_title)\n",
    "    title_passages_ranked = [ rank_bioasq_passage(doc_id, question, chunk, 'title') for chunk in chunks_title ]\n",
    "    chunks_abstract = ranking.split_chunks(doc_abstract)\n",
    "    abstract_passages_ranked = [ rank_bioasq_passage(doc_id, question, chunk, 'abstract') for chunk in chunks_abstract ]\n",
    "    return title_passages_ranked + abstract_passages_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55320f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rank_answer_candidates(question, docs):\n",
    "    snippets = []\n",
    "    w = np.linspace(0,0.0,len(docs))[::-1]\n",
    "    count_pass1 = 0\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_id = doc.replace(globals.BIOASQ.doc_relative_url,'')\n",
    "        doc_id, doc_title, doc_abstract = bioasq_util.get_doc(doc_id, index_name)\n",
    "        #doc_id = doc[0].replace(doc_relative_url,'')\n",
    "        #doc_title = doc[1]\n",
    "        #doc_abstract = doc[2]\n",
    "        snippets_ranked = rank_document(question, doc_id, doc_title, doc_abstract)\n",
    "        count_pass1 += len(snippets_ranked)\n",
    "        snippets_ranked = [ s for s in  snippets_ranked if s['score'] >= 0.6 ]\n",
    "        for s in snippets_ranked:\n",
    "            s['score'] = s['score'] + w[i]\n",
    "        snippets.extend(snippets_ranked)\n",
    "    #print('Percentage of remaining passages {}'.format(len(snippets)/count_pass1))\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94ae076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                    | 0/100 [00:00<?, ?it/s]/home/andresr/.local/lib/python3.9/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "  0%|                                                                                                                                                                                                                    | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Error in query: ', '9507713')\n",
      "'title'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m test_batch_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(batch_file[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm(test_batch_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m], position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     snippets \u001b[38;5;241m=\u001b[39m \u001b[43mextract_rank_answer_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     snippets_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(snippets, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i: (i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m snippets_sorted\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mextract_rank_answer_candidates\u001b[0;34m(question, docs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs):\n\u001b[1;32m      6\u001b[0m     doc_id \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m.\u001b[39mBIOASQ\u001b[38;5;241m.\u001b[39mdoc_relative_url,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     doc_id, doc_title, doc_abstract \u001b[38;5;241m=\u001b[39m bioasq_util\u001b[38;5;241m.\u001b[39mget_doc(doc_id, index_name)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#doc_id = doc[0].replace(doc_relative_url,'')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#doc_title = doc[1]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#doc_abstract = doc[2]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     snippets_ranked \u001b[38;5;241m=\u001b[39m rank_document(question, doc_id, doc_title, doc_abstract)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import bioasq_util\n",
    "\n",
    "gs_google_docs = globals.PATH.eval_home + '/examples/aueb_google_docs/aueb_nlp-bioasq6b-submissions/'\n",
    "index_name = globals.BIOASQ.index\n",
    "\n",
    "#evaluate over aueb documents\n",
    "test_batch_docs = [ #('','8b5_ES_30_full.json')\n",
    "                ('6B1_golden.json', gs_google_docs+'1-aueb-nlp-4.json'),\n",
    "                ('6B2_golden.json', gs_google_docs+'2-aueb-nlp-4.json'),\n",
    "                ('6B3_golden.json', gs_google_docs+'3-aueb-nlp-4.json'),\n",
    "                ('6B4_golden.json', gs_google_docs+'4-aueb-nlp-4.json'),\n",
    "                ('6B5_golden.json', gs_google_docs+'5-aueb-nlp-4.json')\n",
    "               ]\n",
    "    \n",
    "df = pd.DataFrame(columns=('batch', 'Mean precision', 'Recall', 'F-Measure', 'MAP', 'GMAP'))\n",
    "\n",
    "for i, batch_file in enumerate(test_batch_docs):\n",
    "    test_batch_json = json.load(open(batch_file[1]))\n",
    "    for sample in tqdm(test_batch_json['questions'], position=0):\n",
    "        snippets = extract_rank_answer_candidates(sample['body'], sample['documents'])\n",
    "        snippets_sorted = sorted(snippets, key = lambda i: (i['score']), reverse=True)\n",
    "        sample['snippets'] = snippets_sorted\n",
    "        sample['documents'] = [ d[0] for d in sample['documents'] ][0:10]\n",
    "        sample['documents'] = sample['documents'][0:10]\n",
    "    submission = test_batch_json.copy()\n",
    "    for q in submission['questions']:\n",
    "        for s in q['snippets']:\n",
    "            del s['score']\n",
    "    submission_file_name =  working_folder + \"/\" + model_id + '_'+batch_file[1].split('/')[-1]\n",
    "    json.dump(submission, open(submission_file_name, 'w'))\n",
    "    docs_score, pass_score = bioasq_eval.get_scores_phaseA(batch_file[0], submission, path_home=eval_home)\n",
    "    print('Document Scores',docs_score)\n",
    "    print('Passage Scores',pass_score)\n",
    "    df.loc[i] = [ batch_file[0].split('.')[0] + '_' + batch_file[1].split('/')[-1].split('.')[0] ] + pass_score\n",
    "\n",
    "df.to_csv(working_folder + \"/\" + model_id+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0b5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
